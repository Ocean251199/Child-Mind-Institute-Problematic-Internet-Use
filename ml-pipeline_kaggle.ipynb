{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow import fs\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combine_parquet(source_folder, output_file):\n",
    "    \"\"\"\n",
    "    Combine Parquet files from a nested folder structure into a single Parquet file.\n",
    "    The files are identified based on the folder name containing 'id='.\n",
    "\n",
    "    Args:\n",
    "        source_folder (str): Path to the folder containing subfolders with Parquet files.\n",
    "        output_file (str): Path where the combined Parquet file will be saved.\n",
    "    \"\"\"\n",
    "        # List of columns to be written to the combined file\n",
    "    columns = ['id', 'relative_date_PCIAT', 'time_formatted', 'enmo_mg']\n",
    "\n",
    "    # Initialize a flag to determine if the output file exists (for schema handling)\n",
    "    first_file = True\n",
    "\n",
    "    # Loop through all subdirectories in the source folder\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):  # Check if it's a Parquet file\n",
    "                try:\n",
    "                    # Extract the part of the folder name after 'id='\n",
    "                    folder_name = os.path.basename(root)\n",
    "                    if \"id=\" in folder_name:\n",
    "                        # Parse the id from the folder name\n",
    "                        id_value = folder_name.split(\"id=\")[1]\n",
    "                        file_path = os.path.join(root, file)\n",
    "\n",
    "                        # Read the Parquet file\n",
    "                        print(f\"Processing file: {file_path}\")\n",
    "                        df = pd.read_parquet(file_path)\n",
    "\n",
    "                        # Log available columns\n",
    "                        print(f\"Columns in {file_path}: {df.columns.tolist()}\")\n",
    "\n",
    "                        # Ensure the DataFrame contains the necessary columns for transformation\n",
    "                        # Add the 'id' column if it's not already present (derived from the folder name)\n",
    "                        df['id'] = id_value\n",
    "\n",
    "                        # Add the 'time_formatted' column if it needs to be created\n",
    "                        if 'time_of_day' in df.columns:\n",
    "                            df['time_formatted'] = pd.to_datetime(df['time_of_day'] / 1000000000, unit='s').dt.strftime('%H:%M:%S')\n",
    "                        else:\n",
    "                            df['time_formatted'] = pd.NA  # Create it as NaN if not available\n",
    "\n",
    "                        # Add the 'enmo_mg' column if 'enmo' exists in the original file\n",
    "                        if 'enmo' in df.columns:\n",
    "                            df['enmo_mg'] = df['enmo'] * 1000\n",
    "                        else:\n",
    "                            df['enmo_mg'] = pd.NA  # Create it as NaN if 'enmo' is missing\n",
    "\n",
    "                        # Filter out non-wear entries if 'non-wear_flag' exists\n",
    "                        if 'non-wear_flag' in df.columns:\n",
    "                            df = df[df['non-wear_flag'] == 0]\n",
    "\n",
    "                        # Keep only necessary columns (add them if needed)\n",
    "                        df = df[columns]\n",
    "\n",
    "                        # Convert to a Parquet table for writing\n",
    "                        table = pa.Table.from_pandas(df)\n",
    "\n",
    "                        # Write to the Parquet file incrementally\n",
    "                        if first_file:\n",
    "                            pq.write_table(table, output_file)\n",
    "                            first_file = False\n",
    "                        else:\n",
    "                            # Append to the Parquet file\n",
    "                            with pq.ParquetWriter(output_file, table.schema, use_dictionary=True) as writer:\n",
    "                                writer.write_table(table)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    print(f\"All files have been processed and combined into {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def modify_parquet_with_intensity(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Modifies a Parquet file by adding an 'intensity' column based on 'enmo_mg' values.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file (str): Path to the input Parquet file.\n",
    "        output_file (str): Path to save the modified Parquet file.\n",
    "    \"\"\"\n",
    "    # Step 1: Open the Parquet file and create a ParquetReader\n",
    "    reader = pq.ParquetFile(input_file)\n",
    "\n",
    "    # Step 2: Create an empty list to collect modified rows\n",
    "    modified_rows = []\n",
    "\n",
    "    # Iterate over row groups in the Parquet file\n",
    "    for i in range(reader.num_row_groups):\n",
    "        # Read each row group into a PyArrow Table\n",
    "        table = reader.read_row_group(i)\n",
    "        \n",
    "        # Convert the table to a pandas DataFrame\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Step 3: Add the conditional column based on 'enmo_mg'\n",
    "        def categorize_intensity(row):\n",
    "            if row['enmo_mg'] <= 50:\n",
    "                return 'still'\n",
    "            elif row['enmo_mg'] <= 200:\n",
    "                return 'light'\n",
    "            elif row['enmo_mg'] <= 500:\n",
    "                return 'moderate'\n",
    "            elif row['enmo_mg'] <= 750:\n",
    "                return 'high'\n",
    "            elif row['enmo_mg'] > 750:\n",
    "                return 'vigorous'\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        # Apply the function to each row to create the 'intensity' column\n",
    "        df['intensity'] = df.apply(categorize_intensity, axis=1)\n",
    "        \n",
    "        # Convert the modified DataFrame back to a PyArrow Table and append it to the list\n",
    "        modified_rows.append(pa.Table.from_pandas(df))\n",
    "\n",
    "    # Step 4: Concatenate all modified tables into one large table\n",
    "    final_table = pa.concat_tables(modified_rows)\n",
    "\n",
    "    # Step 5: Write the modified table to a new Parquet file\n",
    "    pq.write_table(final_table, output_file)\n",
    "\n",
    "    print(f\"Parquet file saved with the new column at {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_intensity_category(input_file, output_file):\n",
    "\n",
    "    # Read the Parquet file\n",
    "    df = pd.read_parquet(input_file)\n",
    "\n",
    "    # Filter only relevant intensity levels\n",
    "    relevant_intensities = ['still', 'light', 'moderate', 'high', 'vigorous']\n",
    "\n",
    "    # Filter rows with relevant intensities\n",
    "    filtered_df = df[df['intensity'].isin(relevant_intensities)].copy()\n",
    "\n",
    "    # Calculate the number of rows for each intensity level per ID and date\n",
    "    result = (\n",
    "        filtered_df.groupby(['id', 'relative_date_PCIAT', 'intensity'])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "        .pivot(index=['id', 'relative_date_PCIAT'], columns='intensity', values='count')\n",
    "        .fillna(0)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Convert counts to minutes (5 seconds per row)\n",
    "    result['still'] = result.get('still', 0) * 5 / 60\n",
    "    result['light'] = result.get('light', 0) * 5 / 60\n",
    "    result['moderate'] = result.get('moderate', 0) * 5 / 60\n",
    "    result['high'] = result.get('high', 0) * 5 / 60\n",
    "    result['vigorous'] = result.get('vigorous', 0) * 5 / 60\n",
    "\n",
    "    # Rename columns to match required output format\n",
    "    result.rename(columns={\n",
    "        'still': 'total_time_still',\n",
    "        'light': 'total_time_light',\n",
    "        'moderate': 'total_time_moderate',\n",
    "        'high': 'total_time_high',\n",
    "        'vigorous': 'total_time_vigorous'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Ensure missing columns are included with zeros\n",
    "    for col in [\n",
    "        'total_time_still', 'total_time_light', \n",
    "        'total_time_moderate', 'total_time_high', \n",
    "        'total_time_vigorous'\n",
    "    ]:\n",
    "        if col not in result.columns:\n",
    "            result[col] = 0\n",
    "\n",
    "    # Save the result to CSV\n",
    "    result.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_average_times(input_csv, output_csv):\n",
    "\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(input_csv, delimiter=\",\", encoding=\"utf-8\")\n",
    "    print(\"Input CSV loaded successfully.\")\n",
    "    print(df.head())  # Preview the input data (optional)\n",
    "\n",
    "    # Group by 'id' and calculate the mean for total_time columns across all dates\n",
    "    averages_per_id = (\n",
    "        df.groupby('id', as_index=False)\n",
    "        [['total_time_still', 'total_time_light','total_time_moderate', 'total_time_high', 'total_time_vigorous']]\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    # Rename the columns to reflect average and unit\n",
    "    averages_per_id.rename(columns={\n",
    "        'total_time_still':'avg_time_still_(min/day)',\n",
    "        'total_time_light':'avg_time_light_(min/day)',\n",
    "        'total_time_moderate': 'avg_time_moderate_(min/day)',\n",
    "        'total_time_high': 'avg_time_high_(min/day)',\n",
    "        'total_time_vigorous': 'avg_time_vigorous_(min/day)'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Preview the result (optional)\n",
    "    print(\"Averages calculated successfully.\")\n",
    "    print(averages_per_id.head())\n",
    "\n",
    "    # Save the result to a new CSV file\n",
    "    averages_per_id.to_csv(output_csv, index=False)\n",
    "    print(f\"Output CSV saved at {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def merge_data_train(csv_file_path, parquet_combined_file_path, save_file_path):\n",
    "    \"\"\"\n",
    "    Processes the data by reading, transforming, and saving it as described in the M language script.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file_path (str): Path to the train/test CSV file.\n",
    "    parquet_combined_file_path (str): Path to the average data CSV file.\n",
    "    save_file_path (str): Path to save the processed data CSV file.\n",
    "    \"\"\"\n",
    "    # Read the train and average CSV files\n",
    "    train_df = pd.read_csv(csv_file_path)\n",
    "    avg_df = pd.read_csv(parquet_combined_file_path)\n",
    "\n",
    "    # Select relevant columns\n",
    "    relevant_columns = [\n",
    "        \"id\", \"Basic_Demos-Age\", \"Basic_Demos-Sex\", \"CGAS-CGAS_Score\",\n",
    "        \"Physical-BMI\", \"Physical-Waist_Circumference\", \"Physical-Diastolic_BP\",\n",
    "        \"Physical-HeartRate\", \"Physical-Systolic_BP\", \"Fitness_Endurance-Time_Mins\",\n",
    "        \"Fitness_Endurance-Time_Sec\", \"FGC-FGC_CU\", \"FGC-FGC_GSND\", \"FGC-FGC_GSD\",\n",
    "        \"FGC-FGC_PU\", \"FGC-FGC_SRL\", \"FGC-FGC_SRR\", \"FGC-FGC_TL\", \"BIA-BIA_Activity_Level_num\",\n",
    "        \"BIA-BIA_BMC\", \"BIA-BIA_BMR\", \"BIA-BIA_DEE\", \"BIA-BIA_ECW\", \"BIA-BIA_FFM\",\n",
    "        \"BIA-BIA_FMI\", \"BIA-BIA_Fat\", \"BIA-BIA_ICW\", \"BIA-BIA_LDM\", \"BIA-BIA_LST\",\n",
    "        \"BIA-BIA_SMM\", \"BIA-BIA_TBW\", \"SDS-SDS_Total_T\", \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "        \"sii\"\n",
    "    ]\n",
    "    train_df = train_df[relevant_columns]\n",
    "\n",
    "    # Add a new column for endurance time in seconds\n",
    "    train_df[\"num_endure_sec\"] = train_df[\"Fitness_Endurance-Time_Mins\"] * 60 + train_df[\"Fitness_Endurance-Time_Sec\"]\n",
    "\n",
    "    columns_to_remove = [\"Fitness_Endurance-Time_Mins\", \"Fitness_Endurance-Time_Sec\"]\n",
    "    train_df = train_df.drop(columns=columns_to_remove)\n",
    "\n",
    "    # Rename columns\n",
    "    column_renames = {\n",
    "        \"Basic_Demos-Age\": \"age\", \"Basic_Demos-Sex\": \"sex\", \"CGAS-CGAS_Score\": \"num_cgas\",\n",
    "        \"Physical-BMI\": \"num_bmi\", \"SDS-SDS_Total_T\": \"num_sleep_loss\",\n",
    "        \"PreInt_EduHx-computerinternet_hoursday\": \"cat_internet_use\",\n",
    "        \"BIA-BIA_TBW\": \"num_tbm\", \"BIA-BIA_SMM\": \"num_smm\", \"BIA-BIA_LST\": \"num_lst\",\n",
    "        \"BIA-BIA_LDM\": \"num_ldm\", \"BIA-BIA_ICW\": \"num_icw\", \"BIA-BIA_Fat\": \"num_fat\",\n",
    "        \"BIA-BIA_FMI\": \"num_fmi\", \"BIA-BIA_FFM\": \"num_ffm\", \"BIA-BIA_ECW\": \"num_ecw\",\n",
    "        \"BIA-BIA_DEE\": \"num_dee\", \"BIA-BIA_BMR\": \"num_bmr\", \"BIA-BIA_Activity_Level_num\": \"cat_activity_level\",\n",
    "        \"BIA-BIA_BMC\": \"num_bmc\", \"FGC-FGC_TL\": \"num_tl\", \"FGC-FGC_SRR\": \"num_srr\",\n",
    "        \"FGC-FGC_SRL\": \"num_srl\", \"FGC-FGC_PU\": \"num_pu\", \"FGC-FGC_GSD\": \"num_gsd\",\n",
    "        \"FGC-FGC_GSND\": \"num_gsnd\", \"FGC-FGC_CU\": \"num_cu\",\n",
    "        \"Physical-Waist_Circumference\": \"num_wc\", \"Physical-Diastolic_BP\": \"num_dbp\",\n",
    "        \"Physical-Systolic_BP\": \"num_sbp\", \"Physical-HeartRate\": \"num_hr\"\n",
    "    }\n",
    "    train_df.rename(columns=column_renames, inplace=True)\n",
    "\n",
    "    # Merge with average data\n",
    "    result_df = train_df.merge(avg_df, on=\"id\", how=\"left\")\n",
    "\n",
    "    # Remove rows where 'sii' is null\n",
    "    result_df = result_df.dropna(subset=[\"sii\"])\n",
    "\n",
    "    # Save the processed DataFrame\n",
    "    result_df.to_csv(save_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def merge_data_test(csv_file_path, parquet_combined_file_path, save_file_path):\n",
    "    \"\"\"\n",
    "    Processes the data by reading, transforming, and saving it as described in the M language script.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file_path (str): Path to the train/test CSV file.\n",
    "    parquet_combined_file_path (str): Path to the average data CSV file.\n",
    "    save_file_path (str): Path to save the processed data CSV file.\n",
    "    \"\"\"\n",
    "    # Read the train and average CSV files\n",
    "    test_df = pd.read_csv(csv_file_path)\n",
    "    avg_df = pd.read_csv(parquet_combined_file_path)\n",
    "\n",
    "    # Select relevant columns\n",
    "    relevant_columns = [\n",
    "        \"id\", \"Basic_Demos-Age\", \"Basic_Demos-Sex\", \"CGAS-CGAS_Score\",\n",
    "        \"Physical-BMI\", \"Physical-Waist_Circumference\", \"Physical-Diastolic_BP\",\n",
    "        \"Physical-HeartRate\", \"Physical-Systolic_BP\", \"Fitness_Endurance-Time_Mins\",\n",
    "        \"Fitness_Endurance-Time_Sec\", \"FGC-FGC_CU\", \"FGC-FGC_GSND\", \"FGC-FGC_GSD\",\n",
    "        \"FGC-FGC_PU\", \"FGC-FGC_SRL\", \"FGC-FGC_SRR\", \"FGC-FGC_TL\", \"BIA-BIA_Activity_Level_num\",\n",
    "        \"BIA-BIA_BMC\", \"BIA-BIA_BMR\", \"BIA-BIA_DEE\", \"BIA-BIA_ECW\", \"BIA-BIA_FFM\",\n",
    "        \"BIA-BIA_FMI\", \"BIA-BIA_Fat\", \"BIA-BIA_ICW\", \"BIA-BIA_LDM\", \"BIA-BIA_LST\",\n",
    "        \"BIA-BIA_SMM\", \"BIA-BIA_TBW\", \"SDS-SDS_Total_T\", \"PreInt_EduHx-computerinternet_hoursday\"\n",
    "    ]\n",
    "    test_df = test_df[relevant_columns]\n",
    "\n",
    "    # Add a new column for endurance time in seconds\n",
    "    test_df[\"num_endure_sec\"] = test_df[\"Fitness_Endurance-Time_Mins\"] * 60 + test_df[\"Fitness_Endurance-Time_Sec\"]\n",
    "\n",
    "    columns_to_remove = [\"Fitness_Endurance-Time_Mins\", \"Fitness_Endurance-Time_Sec\"]\n",
    "    test_df = test_df.drop(columns=columns_to_remove)\n",
    "\n",
    "    # Rename columns\n",
    "    column_renames = {\n",
    "        \"Basic_Demos-Age\": \"age\", \"Basic_Demos-Sex\": \"sex\", \"CGAS-CGAS_Score\": \"num_cgas\",\n",
    "        \"Physical-BMI\": \"num_bmi\", \"SDS-SDS_Total_T\": \"num_sleep_loss\",\n",
    "        \"PreInt_EduHx-computerinternet_hoursday\": \"cat_internet_use\",\n",
    "        \"BIA-BIA_TBW\": \"num_tbm\", \"BIA-BIA_SMM\": \"num_smm\", \"BIA-BIA_LST\": \"num_lst\",\n",
    "        \"BIA-BIA_LDM\": \"num_ldm\", \"BIA-BIA_ICW\": \"num_icw\", \"BIA-BIA_Fat\": \"num_fat\",\n",
    "        \"BIA-BIA_FMI\": \"num_fmi\", \"BIA-BIA_FFM\": \"num_ffm\", \"BIA-BIA_ECW\": \"num_ecw\",\n",
    "        \"BIA-BIA_DEE\": \"num_dee\", \"BIA-BIA_BMR\": \"num_bmr\", \"BIA-BIA_Activity_Level_num\": \"cat_activity_level\",\n",
    "        \"BIA-BIA_BMC\": \"num_bmc\", \"FGC-FGC_TL\": \"num_tl\", \"FGC-FGC_SRR\": \"num_srr\",\n",
    "        \"FGC-FGC_SRL\": \"num_srl\", \"FGC-FGC_PU\": \"num_pu\", \"FGC-FGC_GSD\": \"num_gsd\",\n",
    "        \"FGC-FGC_GSND\": \"num_gsnd\", \"FGC-FGC_CU\": \"num_cu\",\n",
    "        \"Physical-Waist_Circumference\": \"num_wc\", \"Physical-Diastolic_BP\": \"num_dbp\",\n",
    "        \"Physical-Systolic_BP\": \"num_sbp\", \"Physical-HeartRate\": \"num_hr\"\n",
    "    }\n",
    "    test_df.rename(columns=column_renames, inplace=True)\n",
    "\n",
    "    # Merge with average data\n",
    "    result_df = test_df.merge(avg_df, on=\"id\", how=\"left\")\n",
    "\n",
    "    # Save the processed DataFrame\n",
    "    result_df.to_csv(save_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def impute_missing_values(input_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, fills missing values in numerical and categorical columns,\n",
    "    and saves the processed data to another CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    input_csv_path (str): Path to the input CSV file.\n",
    "    output_csv_path (str): Path to save the processed CSV file.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Identify numerical and categorical columns based on prefixes\n",
    "    numerical_columns = [col for col in df.columns if col.startswith('num_')]\n",
    "    categorical_columns = [col for col in df.columns if col.startswith('cat_')]\n",
    "\n",
    "    # Function to fill missing data\n",
    "    def impute_data(df):\n",
    "        # Impute numerical columns\n",
    "        for col in numerical_columns:\n",
    "            # Group by age and sex, then calculate the mean for each group\n",
    "            # Use transform and fallback to overall mean if group data is missing\n",
    "            df[col] = df.groupby(['age', 'sex'])[col].transform(\n",
    "                lambda x: x.fillna(x.mean() if not x.isnull().all() else df[col].mean())\n",
    "            )\n",
    "\n",
    "        # Impute categorical columns\n",
    "        for col in categorical_columns:\n",
    "            # Group by age and sex, then calculate the mode for each group\n",
    "            # Use transform and fallback to overall mode if group data is missing\n",
    "            df[col] = df.groupby(['age', 'sex'])[col].transform(\n",
    "                lambda x: x.fillna(x.mode()[0] if not x.mode().empty else df[col].mode()[0])\n",
    "            )\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Apply the imputation function\n",
    "    df_imputed = impute_data(df)\n",
    "\n",
    "    # Save the imputed data back to a CSV file\n",
    "    df_imputed.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Data has been imputed and saved to {output_csv_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# os.path.join(working_dir, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change this part depend on where to run, maybe run on Kaggle or local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# input_dir = r\"E:\\MachineLearning\\inputs\"\n",
    "# working_dir = r\"E:\\MachineLearning\\working\"\n",
    "input_dir = r\"/kaggle/input/child-mind-institute-problematic-internet-use\"\n",
    "working_dir = r\"/kaggle/working\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "parquet_src_train = r'series_train.parquet'\n",
    "combined_train = r'combined_train.parquet'\n",
    "intensity_per_day_train = r'intensity_pd_train.csv'\n",
    "avg_intensity_pd_train = r'avg_int_pd_train.csv'\n",
    "train_csv = r'train.csv'\n",
    "merged_train_data = r'merged_train.csv'\n",
    "imputed_train_data = r'imputed_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "combine_parquet(os.path.join(input_dir, parquet_src_train), os.path.join(working_dir, combined_train))\n",
    "modify_parquet_with_intensity(os.path.join(working_dir, combined_train),os.path.join(working_dir, combined_train))\n",
    "calculate_intensity_category(os.path.join(working_dir, combined_train),os.path.join(working_dir, intensity_per_day_train))\n",
    "calculate_average_times(os.path.join(working_dir, intensity_per_day_train),os.path.join(working_dir, avg_intensity_pd_train))\n",
    "merge_data_train(os.path.join(input_dir, train_csv), os.path.join(working_dir, avg_intensity_pd_train), os.path.join(working_dir, merged_train_data))\n",
    "# impute_missing_values(os.path.join(working_dir, merged_train_data), os.path.join(working_dir, imputed_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "parquet_src_test = r'series_test.parquet'\n",
    "combined_test = r'combined_test.parquet'\n",
    "intensity_per_day_test = r'intensity_pd_test.csv'\n",
    "avg_intensity_pd_test = r'avg_int_pd_test.csv'\n",
    "test_csv = r'test.csv'\n",
    "merged_test_data = r'merged_test.csv'\n",
    "imputed_test_data = r'imputed_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "combine_parquet(os.path.join(input_dir, parquet_src_test), os.path.join(working_dir, combined_test))\n",
    "modify_parquet_with_intensity(os.path.join(working_dir, combined_test),os.path.join(working_dir, combined_test))\n",
    "calculate_intensity_category(os.path.join(working_dir, combined_test),os.path.join(working_dir, intensity_per_day_test))\n",
    "calculate_average_times(os.path.join(working_dir, intensity_per_day_test),os.path.join(working_dir, avg_intensity_pd_test))\n",
    "merge_data_test(os.path.join(input_dir, test_csv), os.path.join(working_dir, avg_intensity_pd_test), os.path.join(working_dir, merged_test_data))\n",
    "# impute_missing_values(os.path.join(working_dir, merged_test_data), os.path.join(working_dir, imputed_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML part start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow import fs\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# working_dir = r\"E:\\MachineLearning\\working\"\n",
    "working_dir = r\"/kaggle/working\"\n",
    "train_data_path = r'merged_train.csv'\n",
    "test_data_path = r'merged_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "train_data = pd.read_csv(os.path.join(working_dir, train_data_path))  \n",
    "test_data = pd.read_csv(os.path.join(working_dir, test_data_path))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "columns_to_convert_train = [\"cat_internet_use\", \"cat_activity_level\", \"sii\"]\n",
    "for col in columns_to_convert_train:\n",
    "    train_data[col] = train_data[col].astype(int)\n",
    "\n",
    "columns_to_convert_test = [\"cat_internet_use\", \"cat_activity_level\"]\n",
    "for col in columns_to_convert_test:\n",
    "    test_data[col] = test_data[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data.drop_duplicates(subset='id', keep='first')\n",
    "test_data = test_data.drop_duplicates(subset='id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split the train data into X (features) and y (target)\n",
    "X_train = train_data.drop(columns=['id','sii'])  # Features (exclude target column)\n",
    "y_train = train_data['sii']  # Target variable (0-3 categories)\n",
    "\n",
    "# Split the test data into X (features) and y (target)\n",
    "X_test = test_data.drop(columns=['id'])  # Features (exclude target column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "# Train the Random Forest model (here I picked number 69 hehe)\n",
    "model = lgb.LGBMClassifier(n_estimators = 500, max_depth = 10, num_leaves= 50, learning_rate= 0.01, class_weight= 'balanced')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_test = model.predict(X_test)\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Assign predictions to the test_data DataFrame\n",
    "test_data['pred_sii'] = y_pred_test\n",
    "\n",
    "# Select only the required columns for the output\n",
    "output_df = test_data[['id', 'pred_sii']].copy()\n",
    "\n",
    "# Rename the column to 'sii'\n",
    "output_df.rename(columns={'pred_sii': 'sii'}, inplace=True)\n",
    "\n",
    "# Sort by the 'id' column\n",
    "output_df.sort_values(by='id', inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv('/kaggle/working/submission.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
